{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbdef6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the libraries\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5a61f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the working directory\n",
    "os.chdir(r\"D:\\Gene_Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c214f76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# example sentence\n",
    "sentence = \"The wide road shimmered in the hot sun\"\n",
    "tokens = list(sentence.lower().split())\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1f16dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary index\n",
    "vocab, index = {}, 1  # start indexing from 1\n",
    "vocab['<pad>'] = 0  # add a padding token\n",
    "for token in tokens:\n",
    "  if token not in vocab:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8634cae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"
     ]
    }
   ],
   "source": [
    "# inverse vocabulary\n",
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9541dc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 1, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# map example sentence to tokens\n",
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfc3a962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "# generate skip-grams\n",
    "# i.e positive occurences\n",
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      example_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0)\n",
    "print(len(positive_skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04af80bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5): (road, in)\n",
      "(6, 1): (hot, the)\n",
      "(1, 5): (the, in)\n",
      "(6, 5): (hot, in)\n",
      "(5, 3): (in, road)\n",
      "(1, 7): (the, sun)\n",
      "(1, 4): (the, shimmered)\n",
      "(3, 4): (road, shimmered)\n",
      "(4, 1): (shimmered, the)\n",
      "(1, 3): (the, road)\n",
      "(4, 3): (shimmered, road)\n",
      "(2, 1): (wide, the)\n",
      "(5, 6): (in, hot)\n",
      "(3, 2): (road, wide)\n",
      "(4, 5): (shimmered, in)\n",
      "(1, 2): (the, wide)\n",
      "(5, 1): (in, the)\n",
      "(3, 1): (road, the)\n",
      "(7, 1): (sun, the)\n",
      "(1, 6): (the, hot)\n",
      "(2, 4): (wide, shimmered)\n",
      "(6, 7): (hot, sun)\n",
      "(5, 4): (in, shimmered)\n",
      "(4, 2): (shimmered, wide)\n",
      "(2, 3): (wide, road)\n",
      "(7, 6): (sun, hot)\n"
     ]
    }
   ],
   "source": [
    "# see some example\n",
    "for target, context in positive_skip_grams:\n",
    "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab3f6cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d5cca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 1 4 3], shape=(4,), dtype=int64)\n",
      "['wide', 'the', 'shimmered', 'road']\n"
     ]
    }
   ],
   "source": [
    "# Get target and context words for one positive skip-gram.\n",
    "target_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "# Set the number of negative samples per positive context.\n",
    "num_ns = 4\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
    "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
    "    num_sampled=num_ns,  # number of negative context words to sample\n",
    "    unique=True,  # all the negative samples should be unique\n",
    "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
    "    seed=SEED,  # seed for reproducibility\n",
    "    name=\"negative_sampling\"  # name of this operation\n",
    ")\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30dad87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dimension so you can use concatenation (on the next step).\n",
    "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d579a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat positive context word with negative sampled words.\n",
    "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "# Label first context word as 1 (positive) followed by num_ns 0s (negative).\n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47118960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
       "array([[5],\n",
       "       [2],\n",
       "       [1],\n",
       "       [4],\n",
       "       [3]], dtype=int64)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c57b951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape target to shape (1,) and context and label to (num_ns+1,).\n",
    "target = tf.squeeze(target_word)\n",
    "context = tf.squeeze(context)\n",
    "label = tf.squeeze(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "017a308f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1910168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([5, 2, 1, 4, 3], dtype=int64)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c369199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 0, 0, 0, 0], dtype=int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02ee22f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=int32, numpy=3>,\n",
       " <tf.Tensor: shape=(5,), dtype=int64, numpy=array([5, 2, 1, 4, 3], dtype=int64)>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae592d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for vocab_size tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=SEED,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      negative_sampling_candidates = tf.expand_dims(\n",
    "          negative_sampling_candidates, 1)\n",
    "\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3ef6ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the unsupervised data\n",
    "unsupervised_gene_data = pd.read_csv(\"all_unsupervised_genes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c355d163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.A.72,MerR,GH23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.A.1,CE4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.A.23,9.A.5,MCPsignal,2.A.21,2.A.22,TetR_N,3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CE4,8.A.5,3.A.1,3.A.1,3.A.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GT51,Peripla_BP_2,3.A.1,3.A.1,9.B.169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence\n",
       "0                                   1.A.72,MerR,GH23\n",
       "1                                          3.A.1,CE4\n",
       "2  3.A.23,9.A.5,MCPsignal,2.A.21,2.A.22,TetR_N,3....\n",
       "3                        CE4,8.A.5,3.A.1,3.A.1,3.A.1\n",
       "4              GT51,Peripla_BP_2,3.A.1,3.A.1,9.B.169"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unsupervised data\n",
    "unsupervised_gene_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cecebcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(771293, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsupervised_gene_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43e60608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the new supervised data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "999f8500",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_supervised = pd.read_csv(\"pul_seq_low_high_substr_year.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "731807b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PULid</th>\n",
       "      <th>sig_gene_seq</th>\n",
       "      <th>low_level_substr</th>\n",
       "      <th>high_level_substr</th>\n",
       "      <th>Pub_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PUL0001</td>\n",
       "      <td>GH1,8.A.49,CE2,GH130,GH130,3.A.1,3.A.1,SBP_bac...</td>\n",
       "      <td>beta-mannan</td>\n",
       "      <td>beta-mannan</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PUL0002</td>\n",
       "      <td>GH16</td>\n",
       "      <td>lichenan</td>\n",
       "      <td>beta-glucan</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PUL0003</td>\n",
       "      <td>GH30_8,GH43_16|CBM6</td>\n",
       "      <td>xylan</td>\n",
       "      <td>xylan</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PUL0004</td>\n",
       "      <td>4.A.1,GH1</td>\n",
       "      <td>glucose,cellobiose,maltose</td>\n",
       "      <td>multiple_substrates</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PUL0005</td>\n",
       "      <td>GH94,GH3</td>\n",
       "      <td>beta-glucan,sophorose,laminaribiose</td>\n",
       "      <td>multiple_substrates</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PULid                                       sig_gene_seq  \\\n",
       "0  PUL0001  GH1,8.A.49,CE2,GH130,GH130,3.A.1,3.A.1,SBP_bac...   \n",
       "1  PUL0002                                               GH16   \n",
       "2  PUL0003                                GH30_8,GH43_16|CBM6   \n",
       "3  PUL0004                                          4.A.1,GH1   \n",
       "4  PUL0005                                           GH94,GH3   \n",
       "\n",
       "                      low_level_substr    high_level_substr Pub_year  \n",
       "0                          beta-mannan          beta-mannan     2019  \n",
       "1                             lichenan         beta-glucan      1996  \n",
       "2                                xylan                xylan     2016  \n",
       "3           glucose,cellobiose,maltose  multiple_substrates     2016  \n",
       "4  beta-glucan,sophorose,laminaribiose  multiple_substrates     2016  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_supervised.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32ff7b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the genes from the supervised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3bc9e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seq_supervised = new_supervised[\"sig_gene_seq\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "65ffdaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the genes in supervised data\n",
    "all_genes_supervised = [gene for seq in all_seq_supervised for gene in str(seq).split(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "388ade95",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seq_unsupervised = unsupervised_gene_data[\"sequence\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dabe53fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the genes in supervised data\n",
    "all_genes_unsupervised = [gene for seq in all_seq_unsupervised for gene in str(seq).split(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7a856882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Counter for the unsupervised genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c1717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newone",
   "language": "python",
   "name": "newone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
